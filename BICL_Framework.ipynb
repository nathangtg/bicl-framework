{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPCTnUaIDjJ5+3D4HYjodQQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathangtg/bicl-framework/blob/main/BICL_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ITIwyjBYN44r",
        "outputId": "41d4cc48-2077-473f-c6ab-fdbc9e750471"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ddc00cc-d8bd-464f-9062-b47522421f6c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ddc00cc-d8bd-464f-9062-b47522421f6c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bicl-framework.zip to bicl-framework.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip bicl-framework.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PLhfPctOK35",
        "outputId": "f0b5f4f5-c50a-491f-be72-b5889467fc0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  bicl-framework.zip\n",
            "   creating: bicl-framework/\n",
            "  inflating: bicl-framework/requirements.txt  \n",
            "   creating: bicl-framework/tests/\n",
            "  inflating: bicl-framework/tests/test_frameworks.py  \n",
            "  inflating: bicl-framework/README.md  \n",
            " extracting: bicl-framework/.gitignore  \n",
            "   creating: bicl-framework/configs/\n",
            "  inflating: bicl-framework/configs/experiment_config.yaml  \n",
            "   creating: bicl-framework/scripts/\n",
            "  inflating: bicl-framework/scripts/run_experiment.py  \n",
            "  inflating: bicl-framework/scripts/plot_results.py  \n",
            "   creating: bicl-framework/.git/\n",
            " extracting: bicl-framework/.git/ORIG_HEAD  \n",
            "  inflating: bicl-framework/.git/config  \n",
            "   creating: bicl-framework/.git/objects/\n",
            "   creating: bicl-framework/.git/objects/68/\n",
            " extracting: bicl-framework/.git/objects/68/e8d1c0cd8040725c12419ba3d4824f22eccaf7  \n",
            "   creating: bicl-framework/.git/objects/3d/\n",
            " extracting: bicl-framework/.git/objects/3d/3e322903917f132569d6361d157101fce31f6d  \n",
            " extracting: bicl-framework/.git/objects/3d/32e66ad772ee4609aad6b08bf60412ae45bc21  \n",
            "   creating: bicl-framework/.git/objects/67/\n",
            " extracting: bicl-framework/.git/objects/67/534a5340c2e3df6bd97ae3580de9c5a721d69c  \n",
            "   creating: bicl-framework/.git/objects/93/\n",
            " extracting: bicl-framework/.git/objects/93/e490c31303aeacc8d0059fbaf640d9c202fc65  \n",
            "   creating: bicl-framework/.git/objects/33/\n",
            " extracting: bicl-framework/.git/objects/33/a00d636d4187ff9d2299f8c07d54873c837de2  \n",
            " extracting: bicl-framework/.git/objects/33/33c961971b6291859e60a938d93a2370d579e5  \n",
            " extracting: bicl-framework/.git/objects/33/79147b09289fa3d345d118dc6675d2ae17afa0  \n",
            "   creating: bicl-framework/.git/objects/05/\n",
            " extracting: bicl-framework/.git/objects/05/a5cb879e4e478f5322c4b2e2ac17158bfb4b3c  \n",
            "   creating: bicl-framework/.git/objects/b3/\n",
            " extracting: bicl-framework/.git/objects/b3/9de6cf79e40347532d1048d6e5a9c2c55f3ee9  \n",
            "   creating: bicl-framework/.git/objects/d8/\n",
            " extracting: bicl-framework/.git/objects/d8/325a29c81dca6ea436f2082be4bab3c27d8b15  \n",
            "   creating: bicl-framework/.git/objects/eb/\n",
            " extracting: bicl-framework/.git/objects/eb/3a4b9a268ea7aabff80c27b5a41d82a300fa3a  \n",
            " extracting: bicl-framework/.git/objects/eb/55431d06dab8076a526483ac08f6a637b9c985  \n",
            "   creating: bicl-framework/.git/objects/c7/\n",
            " extracting: bicl-framework/.git/objects/c7/55c9a1eae927b2dc53b87382bae8626c6d17b8  \n",
            "   creating: bicl-framework/.git/objects/c0/\n",
            " extracting: bicl-framework/.git/objects/c0/543eb2a611d696893492974b4e02f41c29708c  \n",
            "   creating: bicl-framework/.git/objects/fc/\n",
            " extracting: bicl-framework/.git/objects/fc/c54e2fe07ac760488aef1b03d57e2619b778c8  \n",
            "   creating: bicl-framework/.git/objects/fe/\n",
            " extracting: bicl-framework/.git/objects/fe/004eb6b320b5d6492c20e75fbf38f03c220af1  \n",
            " extracting: bicl-framework/.git/objects/fe/99d5a4185ff4580ffc8ca6b7b73cb87a808c08  \n",
            "   creating: bicl-framework/.git/objects/fb/\n",
            " extracting: bicl-framework/.git/objects/fb/8c6ffb02e13b331e3ab482aad09731df0197a5  \n",
            "   creating: bicl-framework/.git/objects/c6/\n",
            " extracting: bicl-framework/.git/objects/c6/dcf805a87e548714159edab924efb7eb41f668  \n",
            "   creating: bicl-framework/.git/objects/27/\n",
            " extracting: bicl-framework/.git/objects/27/12cb8bea9760e80f7110e00cb32a7102b8aee7  \n",
            "   creating: bicl-framework/.git/objects/4b/\n",
            " extracting: bicl-framework/.git/objects/4b/4c9aa5afc320cafea84915271860b821f8b7da  \n",
            "   creating: bicl-framework/.git/objects/pack/\n",
            "   creating: bicl-framework/.git/objects/87/\n",
            " extracting: bicl-framework/.git/objects/87/3d4c204e6c70018795a7a89eecca60f5deb33a  \n",
            " extracting: bicl-framework/.git/objects/87/9750e70d627989a8b5d958770bfb56f36a0c2d  \n",
            "   creating: bicl-framework/.git/objects/19/\n",
            " extracting: bicl-framework/.git/objects/19/61523a190d8e2eae67dde456645e0e26cae4c0  \n",
            "   creating: bicl-framework/.git/objects/86/\n",
            " extracting: bicl-framework/.git/objects/86/bc2bea050e00fab3c5d5cd937df4bc96295c19  \n",
            "   creating: bicl-framework/.git/objects/43/\n",
            " extracting: bicl-framework/.git/objects/43/8151c743e66e04f22e021f82cc7d7ccb884afb  \n",
            "   creating: bicl-framework/.git/objects/info/\n",
            "   creating: bicl-framework/.git/objects/53/\n",
            " extracting: bicl-framework/.git/objects/53/59a7528cf94047b1a4eb996e9e43b0b4023495  \n",
            "   creating: bicl-framework/.git/objects/3f/\n",
            " extracting: bicl-framework/.git/objects/3f/ddd266c8beef96965785b7682be84b05528922  \n",
            " extracting: bicl-framework/.git/objects/3f/151abe88fa2db530bb7b8bce5d2e1f2a6db885  \n",
            "   creating: bicl-framework/.git/objects/08/\n",
            " extracting: bicl-framework/.git/objects/08/1c4aebcaa7d200f9b7ef346412353867198899  \n",
            " extracting: bicl-framework/.git/objects/08/efd43d299d1f5d4fb657da70689084f26acc54  \n",
            "   creating: bicl-framework/.git/objects/06/\n",
            " extracting: bicl-framework/.git/objects/06/20e5ddd6494fe44a3ad1a81af619e9a3c32cb4  \n",
            "   creating: bicl-framework/.git/objects/bf/\n",
            " extracting: bicl-framework/.git/objects/bf/adb6fdb979b7189719d1a80757e1603ad3113f  \n",
            "   creating: bicl-framework/.git/objects/a7/\n",
            " extracting: bicl-framework/.git/objects/a7/6f8ef84fa05baa7bf89d3194c134744994053e  \n",
            " extracting: bicl-framework/.git/objects/a7/6f680c2562cb0d445e08bd8e98a77e7de77d1a  \n",
            "   creating: bicl-framework/.git/objects/b1/\n",
            " extracting: bicl-framework/.git/objects/b1/b6ff864b756b398c8eaa10cc1658e349741d20  \n",
            " extracting: bicl-framework/.git/objects/b1/81aa29226e30a74dd59f8839bd1109da48aaea  \n",
            "   creating: bicl-framework/.git/objects/a1/\n",
            " extracting: bicl-framework/.git/objects/a1/9518db965870bf3197a68e9f5b0131dc5b71f1  \n",
            "   creating: bicl-framework/.git/objects/c4/\n",
            " extracting: bicl-framework/.git/objects/c4/4d21490d692ba18b1bd78d96597d7bdf26c451  \n",
            "   creating: bicl-framework/.git/objects/cc/\n",
            " extracting: bicl-framework/.git/objects/cc/a91e9c833f46cf774aee7d281cf0df137cedde  \n",
            "   creating: bicl-framework/.git/objects/ff/\n",
            " extracting: bicl-framework/.git/objects/ff/4a339a5e9b1cc983d453091a38f3e3c20e838a  \n",
            "   creating: bicl-framework/.git/objects/ce/\n",
            " extracting: bicl-framework/.git/objects/ce/fe5ecea25cfcfaef1b3e8124c5df1951410a6e  \n",
            " extracting: bicl-framework/.git/HEAD  \n",
            "   creating: bicl-framework/.git/info/\n",
            "  inflating: bicl-framework/.git/info/exclude  \n",
            "   creating: bicl-framework/.git/logs/\n",
            "  inflating: bicl-framework/.git/logs/HEAD  \n",
            "   creating: bicl-framework/.git/logs/refs/\n",
            "   creating: bicl-framework/.git/logs/refs/heads/\n",
            "  inflating: bicl-framework/.git/logs/refs/heads/BICL_Implementation  \n",
            "  inflating: bicl-framework/.git/logs/refs/heads/main  \n",
            "   creating: bicl-framework/.git/logs/refs/remotes/\n",
            "   creating: bicl-framework/.git/logs/refs/remotes/origin/\n",
            "  inflating: bicl-framework/.git/logs/refs/remotes/origin/BICL_Implementation  \n",
            "  inflating: bicl-framework/.git/logs/refs/remotes/origin/main  \n",
            "  inflating: bicl-framework/.git/description  \n",
            "   creating: bicl-framework/.git/hooks/\n",
            "  inflating: bicl-framework/.git/hooks/commit-msg.sample  \n",
            "  inflating: bicl-framework/.git/hooks/pre-rebase.sample  \n",
            "  inflating: bicl-framework/.git/hooks/pre-commit.sample  \n",
            "  inflating: bicl-framework/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: bicl-framework/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: bicl-framework/.git/hooks/pre-receive.sample  \n",
            "  inflating: bicl-framework/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: bicl-framework/.git/hooks/post-update.sample  \n",
            "  inflating: bicl-framework/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: bicl-framework/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: bicl-framework/.git/hooks/pre-push.sample  \n",
            "  inflating: bicl-framework/.git/hooks/update.sample  \n",
            "  inflating: bicl-framework/.git/hooks/push-to-checkout.sample  \n",
            "   creating: bicl-framework/.git/refs/\n",
            "   creating: bicl-framework/.git/refs/heads/\n",
            " extracting: bicl-framework/.git/refs/heads/BICL_Implementation  \n",
            " extracting: bicl-framework/.git/refs/heads/main  \n",
            "   creating: bicl-framework/.git/refs/tags/\n",
            "   creating: bicl-framework/.git/refs/remotes/\n",
            "   creating: bicl-framework/.git/refs/remotes/origin/\n",
            " extracting: bicl-framework/.git/refs/remotes/origin/BICL_Implementation  \n",
            " extracting: bicl-framework/.git/refs/remotes/origin/main  \n",
            "  inflating: bicl-framework/.git/index  \n",
            " extracting: bicl-framework/.git/COMMIT_EDITMSG  \n",
            "  inflating: bicl-framework/.git/FETCH_HEAD  \n",
            "   creating: bicl-framework/src/\n",
            "  inflating: bicl-framework/src/plotting.py  \n",
            "   creating: bicl-framework/src/__pycache__/\n",
            "  inflating: bicl-framework/src/__pycache__/data.cpython-311.pyc  \n",
            "  inflating: bicl-framework/src/__pycache__/model.cpython-311.pyc  \n",
            "  inflating: bicl-framework/src/__pycache__/utils.cpython-311.pyc  \n",
            "  inflating: bicl-framework/src/__pycache__/frameworks.cpython-311.pyc  \n",
            "  inflating: bicl-framework/src/__pycache__/experiment.cpython-311.pyc  \n",
            "  inflating: bicl-framework/src/model.py  \n",
            "  inflating: bicl-framework/src/experiment.py  \n",
            "  inflating: bicl-framework/src/utils.py  \n",
            "  inflating: bicl-framework/src/frameworks.py  \n",
            "  inflating: bicl-framework/src/data.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r bicl-framework/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Io3y90RtOOkM",
        "outputId": "4abb3765-47ea-44bd-c79f-51f56c1eee55"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contourpy==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 3)) (3.18.0)\n",
            "Collecting fonttools==4.58.5 (from -r bicl-framework/requirements.txt (line 4))\n",
            "  Downloading fonttools-4.58.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec==2025.5.1 (from -r bicl-framework/requirements.txt (line 5))\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: iniconfig==2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 6)) (2.1.0)\n",
            "Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: kiwisolver==1.4.8 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 9)) (1.4.8)\n",
            "Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 10)) (3.0.2)\n",
            "Collecting matplotlib==3.10.3 (from -r bicl-framework/requirements.txt (line 11))\n",
            "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: networkx==3.5 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 13)) (3.5)\n",
            "Collecting numpy==2.3.1 (from -r bicl-framework/requirements.txt (line 14))\n",
            "  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==25.0 (from -r bicl-framework/requirements.txt (line 15))\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pandas==2.3.0 (from -r bicl-framework/requirements.txt (line 16))\n",
            "  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow==11.3.0 (from -r bicl-framework/requirements.txt (line 17))\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pluggy==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 18)) (1.6.0)\n",
            "Requirement already satisfied: Pygments==2.19.2 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 19)) (2.19.2)\n",
            "Requirement already satisfied: pyparsing==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 20)) (3.2.3)\n",
            "Collecting pytest==8.4.1 (from -r bicl-framework/requirements.txt (line 21))\n",
            "  Downloading pytest-8.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 22)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 23)) (2025.2)\n",
            "Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 24)) (6.0.2)\n",
            "Collecting scikit-learn==1.7.0 (from -r bicl-framework/requirements.txt (line 25))\n",
            "  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Collecting scipy==1.16.0 (from -r bicl-framework/requirements.txt (line 26))\n",
            "  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 27)) (0.13.2)\n",
            "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 28)) (1.17.0)\n",
            "Collecting sympy==1.14.0 (from -r bicl-framework/requirements.txt (line 29))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: threadpoolctl==3.6.0 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 30)) (3.6.0)\n",
            "Collecting torch==2.7.1 (from -r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchvision==0.22.1 (from -r bicl-framework/requirements.txt (line 32))\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting typing_extensions==4.14.1 (from -r bicl-framework/requirements.txt (line 33))\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: tzdata==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r bicl-framework/requirements.txt (line 34)) (2025.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch==2.7.1->-r bicl-framework/requirements.txt (line 31))\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch==2.7.1->-r bicl-framework/requirements.txt (line 31)) (75.2.0)\n",
            "Downloading fonttools-4.58.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest-8.4.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.5/365.5 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, typing_extensions, triton, sympy, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fsspec, fonttools, scipy, pytest, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, torchvision\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.58.4\n",
            "    Uninstalling fonttools-4.58.4:\n",
            "      Successfully uninstalled fonttools-4.58.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 8.3.5\n",
            "    Uninstalling pytest-8.3.5:\n",
            "      Successfully uninstalled pytest-8.3.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "langchain-core 0.3.67 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "plotnine 0.14.6 requires scipy<1.16.0,>=1.8.0, but you have scipy 1.16.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fonttools-4.58.5 fsspec-2025.5.1 matplotlib-3.10.3 numpy-2.3.1 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 packaging-25.0 pandas-2.3.0 pillow-11.3.0 pytest-8.4.1 scikit-learn-1.7.0 scipy-1.16.0 sympy-1.14.0 torch-2.7.1 torchvision-0.22.1 triton-3.3.1 typing_extensions-4.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "packaging"
                ]
              },
              "id": "11149743fe5d46a6bb8a0e9cbb7b32ee"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bicl-framework/scripts/run_experiment.py --config bicl-framework/configs/debug_config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK3DuaVHR-XH",
        "outputId": "bf88cecb-dafb-43aa-c509-36fca08dc281"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-06 02:22:06,765 [INFO    ] Experiment results will be saved to: results/BICL_CIFAR10_Efficient_Tune_2025-07-06_02-22-06\n",
            "2025-07-06 02:22:06,767 [INFO    ] Saved a copy of the configuration file for this run.\n",
            "2025-07-06 02:22:06,799 [INFO    ] Running on CUDA with random seed 42.\n",
            "2025-07-06 02:22:06,800 [INFO    ] Initializing ExperimentRunner...\n",
            "2025-07-06 02:22:06,800 [INFO    ] ExperimentRunner initialized. Using device: cuda\n",
            "2025-07-06 02:22:06,800 [INFO    ] Starting comprehensive experiment runs...\n",
            "2025-07-06 02:22:06,800 [INFO    ] Generating hyperparameter trials...\n",
            "2025-07-06 02:22:06,800 [INFO    ] Generated 3 unique trials for the experiment.\n",
            "2025-07-06 02:22:06,800 [INFO    ]   - Trial generated: vanilla\n",
            "2025-07-06 02:22:06,800 [INFO    ]   - Trial generated: bicl_beta0.01\n",
            "2025-07-06 02:22:06,800 [INFO    ]   - Trial generated: bicl_beta0.005\n",
            "2025-07-06 02:22:06,800 [INFO    ] --- Starting Statistical Run 1/5 ---\n",
            "2025-07-06 02:22:06,800 [INFO    ] Preparing Split CIFAR-10 benchmark...\n",
            "Run 1 | Trial: vanilla:   0% 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  -> Task 1 Training:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  -> Task 1 Training:   0% 0/20 [00:08<?, ?it/s, train_loss=1.2498, val_loss=1.0501]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:08<02:41,  8.48s/it, train_loss=1.2498, val_loss=1.0501]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:16<02:41,  8.48s/it, train_loss=0.9221, val_loss=0.8392]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:16<02:26,  8.13s/it, train_loss=0.9221, val_loss=0.8392]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:24<02:26,  8.13s/it, train_loss=0.8152, val_loss=0.7853]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:24<02:16,  8.04s/it, train_loss=0.8152, val_loss=0.7853]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:32<02:16,  8.04s/it, train_loss=0.7328, val_loss=0.7533]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:32<02:07,  8.00s/it, train_loss=0.7328, val_loss=0.7533]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:40<02:07,  8.00s/it, train_loss=0.6897, val_loss=0.6874]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:40<01:59,  7.95s/it, train_loss=0.6897, val_loss=0.6874]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:47<01:59,  7.95s/it, train_loss=0.6501, val_loss=0.7969]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:47<01:50,  7.89s/it, train_loss=0.6501, val_loss=0.7969]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:55<01:50,  7.89s/it, train_loss=0.6257, val_loss=0.6978]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [00:55<01:42,  7.85s/it, train_loss=0.6257, val_loss=0.6978]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [01:03<01:42,  7.85s/it, train_loss=0.5976, val_loss=0.6961]\u001b[A\n",
            "  -> Task 1 Training:  40% 8/20 [01:03<01:34,  7.84s/it, train_loss=0.5976, val_loss=0.6961]\u001b[A\n",
            "  -> Task 1 Training:  40% 8/20 [01:11<01:34,  7.84s/it, train_loss=0.5720, val_loss=0.7398]\u001b[A\n",
            "  -> Task 1 Training:  45% 9/20 [01:11<01:26,  7.82s/it, train_loss=0.5720, val_loss=0.7398]\u001b[A\n",
            "  -> Task 1 Training:  45% 9/20 [01:19<01:26,  7.82s/it, train_loss=0.5544, val_loss=0.7619]\u001b[A\n",
            "                                                                                            \u001b[A\n",
            "  -> Task 2 Training:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  -> Task 2 Training:   0% 0/20 [00:07<?, ?it/s, train_loss=1.2670, val_loss=0.7668]\u001b[A\n",
            "  -> Task 2 Training:   5% 1/20 [00:07<02:31,  7.98s/it, train_loss=1.2670, val_loss=0.7668]\u001b[A\n",
            "  -> Task 2 Training:   5% 1/20 [00:15<02:31,  7.98s/it, train_loss=0.6266, val_loss=0.5783]\u001b[A\n",
            "  -> Task 2 Training:  10% 2/20 [00:15<02:23,  7.97s/it, train_loss=0.6266, val_loss=0.5783]\u001b[A\n",
            "  -> Task 2 Training:  10% 2/20 [00:23<02:23,  7.97s/it, train_loss=0.5316, val_loss=0.5779]\u001b[A\n",
            "  -> Task 2 Training:  15% 3/20 [00:23<02:15,  7.96s/it, train_loss=0.5316, val_loss=0.5779]\u001b[A\n",
            "  -> Task 2 Training:  15% 3/20 [00:31<02:15,  7.96s/it, train_loss=0.4717, val_loss=0.5248]\u001b[A\n",
            "  -> Task 2 Training:  20% 4/20 [00:31<02:07,  7.95s/it, train_loss=0.4717, val_loss=0.5248]\u001b[A\n",
            "  -> Task 2 Training:  20% 4/20 [00:39<02:07,  7.95s/it, train_loss=0.4274, val_loss=0.5814]\u001b[A\n",
            "  -> Task 2 Training:  25% 5/20 [00:39<01:58,  7.92s/it, train_loss=0.4274, val_loss=0.5814]\u001b[A\n",
            "  -> Task 2 Training:  25% 5/20 [00:47<01:58,  7.92s/it, train_loss=0.4068, val_loss=0.5325]\u001b[A\n",
            "  -> Task 2 Training:  30% 6/20 [00:47<01:50,  7.87s/it, train_loss=0.4068, val_loss=0.5325]\u001b[A\n",
            "  -> Task 2 Training:  30% 6/20 [00:55<01:50,  7.87s/it, train_loss=0.3731, val_loss=0.5863]\u001b[A\n",
            "  -> Task 2 Training:  35% 7/20 [00:55<01:42,  7.86s/it, train_loss=0.3731, val_loss=0.5863]\u001b[A\n",
            "  -> Task 2 Training:  35% 7/20 [01:03<01:42,  7.86s/it, train_loss=0.3552, val_loss=0.4392]\u001b[A\n",
            "  -> Task 2 Training:  40% 8/20 [01:03<01:34,  7.87s/it, train_loss=0.3552, val_loss=0.4392]\u001b[A\n",
            "  -> Task 2 Training:  40% 8/20 [01:11<01:34,  7.87s/it, train_loss=0.3299, val_loss=0.4597]\u001b[A\n",
            "  -> Task 2 Training:  45% 9/20 [01:11<01:26,  7.86s/it, train_loss=0.3299, val_loss=0.4597]\u001b[A\n",
            "  -> Task 2 Training:  45% 9/20 [01:18<01:26,  7.86s/it, train_loss=0.3141, val_loss=0.4753]\u001b[A\n",
            "  -> Task 2 Training:  50% 10/20 [01:18<01:18,  7.84s/it, train_loss=0.3141, val_loss=0.4753]\u001b[A\n",
            "  -> Task 2 Training:  50% 10/20 [01:26<01:18,  7.84s/it, train_loss=0.3022, val_loss=0.6602]\u001b[A\n",
            "  -> Task 2 Training:  55% 11/20 [01:26<01:10,  7.85s/it, train_loss=0.3022, val_loss=0.6602]\u001b[A\n",
            "  -> Task 2 Training:  55% 11/20 [01:34<01:10,  7.85s/it, train_loss=0.2820, val_loss=0.5080]\u001b[A\n",
            "  -> Task 2 Training:  60% 12/20 [01:34<01:02,  7.85s/it, train_loss=0.2820, val_loss=0.5080]\u001b[A\n",
            "  -> Task 2 Training:  60% 12/20 [01:42<01:02,  7.85s/it, train_loss=0.2668, val_loss=0.4984]\u001b[A\n",
            "                                                                                             \u001b[A2025-07-06 02:25:40,992 [INFO    ]     Trial complete: vanilla. Avg Acc: 0.415, BWT: -0.742, Time: 181.6s\n",
            "Run 1 | Trial: bicl_beta0.01:  33% 1/3 [03:07<06:14, 187.07s/it]\n",
            "  -> Task 1 Training:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  -> Task 1 Training:   0% 0/20 [00:07<?, ?it/s, train_loss=1.2130, val_loss=1.1031]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:07<02:31,  7.96s/it, train_loss=1.2130, val_loss=1.1031]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:15<02:31,  7.96s/it, train_loss=0.9083, val_loss=0.8820]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:15<02:23,  7.96s/it, train_loss=0.9083, val_loss=0.8820]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:23<02:23,  7.96s/it, train_loss=0.8123, val_loss=0.8035]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:23<02:15,  7.96s/it, train_loss=0.8123, val_loss=0.8035]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:31<02:15,  7.96s/it, train_loss=0.7351, val_loss=0.8846]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:31<02:06,  7.91s/it, train_loss=0.7351, val_loss=0.8846]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:39<02:06,  7.91s/it, train_loss=0.6765, val_loss=0.7339]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:39<01:59,  7.96s/it, train_loss=0.6765, val_loss=0.7339]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:47<01:59,  7.96s/it, train_loss=0.6458, val_loss=0.7481]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:47<01:51,  7.94s/it, train_loss=0.6458, val_loss=0.7481]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:55<01:51,  7.94s/it, train_loss=0.6178, val_loss=0.6904]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [00:55<01:43,  7.96s/it, train_loss=0.6178, val_loss=0.6904]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [01:03<01:43,  7.96s/it, train_loss=0.5921, val_loss=0.7167]\u001b[A\n",
            "  -> Task 1 Training:  40% 8/20 [01:03<01:35,  7.93s/it, train_loss=0.5921, val_loss=0.7167]\u001b[A\n",
            "  -> Task 1 Training:  40% 8/20 [01:11<01:35,  7.93s/it, train_loss=0.5649, val_loss=0.6901]\u001b[A\n",
            "  -> Task 1 Training:  45% 9/20 [01:11<01:27,  7.93s/it, train_loss=0.5649, val_loss=0.6901]\u001b[A\n",
            "  -> Task 1 Training:  45% 9/20 [01:19<01:27,  7.93s/it, train_loss=0.5483, val_loss=0.7022]\u001b[A\n",
            "  -> Task 1 Training:  50% 10/20 [01:19<01:19,  7.93s/it, train_loss=0.5483, val_loss=0.7022]\u001b[A\n",
            "  -> Task 1 Training:  50% 10/20 [01:27<01:19,  7.93s/it, train_loss=0.5243, val_loss=0.6759]\u001b[A\n",
            "  -> Task 1 Training:  55% 11/20 [01:27<01:11,  7.95s/it, train_loss=0.5243, val_loss=0.6759]\u001b[A\n",
            "  -> Task 1 Training:  55% 11/20 [01:35<01:11,  7.95s/it, train_loss=0.5207, val_loss=0.6825]\u001b[A\n",
            "  -> Task 1 Training:  60% 12/20 [01:35<01:03,  7.92s/it, train_loss=0.5207, val_loss=0.6825]\u001b[A\n",
            "  -> Task 1 Training:  60% 12/20 [01:43<01:03,  7.92s/it, train_loss=0.4968, val_loss=0.7026]\u001b[A\n",
            "  -> Task 1 Training:  65% 13/20 [01:43<00:55,  7.90s/it, train_loss=0.4968, val_loss=0.7026]\u001b[A\n",
            "  -> Task 1 Training:  65% 13/20 [01:50<00:55,  7.90s/it, train_loss=0.4844, val_loss=0.7114]\u001b[A\n",
            "  -> Task 1 Training:  70% 14/20 [01:50<00:47,  7.89s/it, train_loss=0.4844, val_loss=0.7114]\u001b[A\n",
            "  -> Task 1 Training:  70% 14/20 [01:58<00:47,  7.89s/it, train_loss=0.4626, val_loss=0.7482]\u001b[A\n",
            "  -> Task 1 Training:  75% 15/20 [01:58<00:39,  7.88s/it, train_loss=0.4626, val_loss=0.7482]\u001b[A\n",
            "  -> Task 1 Training:  75% 15/20 [02:06<00:39,  7.88s/it, train_loss=0.4462, val_loss=0.8673]\u001b[A\n",
            "                                                                                             \u001b[A    Computing Importance Weights (Ω) for BICL...\n",
            "\n",
            "  -> Task 2 Training:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  -> Task 2 Training:   0% 0/20 [00:11<?, ?it/s, train_loss=1.2560, val_loss=1.0229]\u001b[A\n",
            "  -> Task 2 Training:   5% 1/20 [00:11<03:39, 11.53s/it, train_loss=1.2560, val_loss=1.0229]\u001b[A\n",
            "  -> Task 2 Training:   5% 1/20 [00:22<03:39, 11.53s/it, train_loss=0.6852, val_loss=0.7603]\u001b[A\n",
            "  -> Task 2 Training:  10% 2/20 [00:22<03:26, 11.45s/it, train_loss=0.6852, val_loss=0.7603]\u001b[A\n",
            "  -> Task 2 Training:  10% 2/20 [00:34<03:26, 11.45s/it, train_loss=0.6545, val_loss=0.7475]\u001b[A\n",
            "  -> Task 2 Training:  15% 3/20 [00:34<03:13, 11.39s/it, train_loss=0.6545, val_loss=0.7475]\u001b[A\n",
            "  -> Task 2 Training:  15% 3/20 [00:45<03:13, 11.39s/it, train_loss=0.6285, val_loss=0.6351]\u001b[A\n",
            "  -> Task 2 Training:  20% 4/20 [00:45<03:02, 11.40s/it, train_loss=0.6285, val_loss=0.6351]\u001b[A\n",
            "  -> Task 2 Training:  20% 4/20 [00:56<03:02, 11.40s/it, train_loss=0.6172, val_loss=0.8923]\u001b[A\n",
            "  -> Task 2 Training:  25% 5/20 [00:56<02:50, 11.37s/it, train_loss=0.6172, val_loss=0.8923]\u001b[A\n",
            "  -> Task 2 Training:  25% 5/20 [01:08<02:50, 11.37s/it, train_loss=0.5960, val_loss=0.6106]\u001b[A\n",
            "  -> Task 2 Training:  30% 6/20 [01:08<02:39, 11.41s/it, train_loss=0.5960, val_loss=0.6106]\u001b[A\n",
            "  -> Task 2 Training:  30% 6/20 [01:19<02:39, 11.41s/it, train_loss=0.5761, val_loss=0.6701]\u001b[A\n",
            "  -> Task 2 Training:  35% 7/20 [01:19<02:28, 11.43s/it, train_loss=0.5761, val_loss=0.6701]\u001b[A\n",
            "  -> Task 2 Training:  35% 7/20 [01:31<02:28, 11.43s/it, train_loss=0.5747, val_loss=0.9487]\u001b[A\n",
            "  -> Task 2 Training:  40% 8/20 [01:31<02:17, 11.43s/it, train_loss=0.5747, val_loss=0.9487]\u001b[A\n",
            "  -> Task 2 Training:  40% 8/20 [01:42<02:17, 11.43s/it, train_loss=0.5691, val_loss=1.2108]\u001b[A\n",
            "  -> Task 2 Training:  45% 9/20 [01:42<02:05, 11.43s/it, train_loss=0.5691, val_loss=1.2108]\u001b[A\n",
            "  -> Task 2 Training:  45% 9/20 [01:54<02:05, 11.43s/it, train_loss=0.5633, val_loss=0.5852]\u001b[A\n",
            "  -> Task 2 Training:  50% 10/20 [01:54<01:54, 11.45s/it, train_loss=0.5633, val_loss=0.5852]\u001b[A\n",
            "  -> Task 2 Training:  50% 10/20 [02:05<01:54, 11.45s/it, train_loss=0.5582, val_loss=0.6911]\u001b[A\n",
            "  -> Task 2 Training:  55% 11/20 [02:05<01:42, 11.42s/it, train_loss=0.5582, val_loss=0.6911]\u001b[A\n",
            "  -> Task 2 Training:  55% 11/20 [02:16<01:42, 11.42s/it, train_loss=0.5494, val_loss=0.6747]\u001b[A\n",
            "  -> Task 2 Training:  60% 12/20 [02:16<01:31, 11.39s/it, train_loss=0.5494, val_loss=0.6747]\u001b[A\n",
            "  -> Task 2 Training:  60% 12/20 [02:28<01:31, 11.39s/it, train_loss=0.5534, val_loss=0.7491]\u001b[A\n",
            "  -> Task 2 Training:  65% 13/20 [02:28<01:19, 11.41s/it, train_loss=0.5534, val_loss=0.7491]\u001b[A\n",
            "  -> Task 2 Training:  65% 13/20 [02:39<01:19, 11.41s/it, train_loss=0.5498, val_loss=1.2629]\u001b[A\n",
            "  -> Task 2 Training:  70% 14/20 [02:39<01:08, 11.41s/it, train_loss=0.5498, val_loss=1.2629]\u001b[A\n",
            "  -> Task 2 Training:  70% 14/20 [02:51<01:08, 11.41s/it, train_loss=0.5345, val_loss=0.8237]\u001b[A\n",
            "                                                                                             \u001b[A    Computing Importance Weights (Ω) for BICL...\n",
            "2025-07-06 02:31:00,543 [INFO    ]     Trial complete: bicl_beta0.01. Avg Acc: 0.383, BWT: -0.747, Time: 298.0s\n",
            "Run 1 | Trial: bicl_beta0.005:  67% 2/3 [08:26<04:25, 265.00s/it]\n",
            "  -> Task 1 Training:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  -> Task 1 Training:   0% 0/20 [00:07<?, ?it/s, train_loss=1.2190, val_loss=1.1383]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:08<02:32,  8.04s/it, train_loss=1.2190, val_loss=1.1383]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:15<02:32,  8.04s/it, train_loss=0.8842, val_loss=0.8799]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:15<02:23,  7.99s/it, train_loss=0.8842, val_loss=0.8799]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:23<02:23,  7.99s/it, train_loss=0.7813, val_loss=1.0534]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:23<02:15,  7.95s/it, train_loss=0.7813, val_loss=1.0534]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:31<02:15,  7.95s/it, train_loss=0.7266, val_loss=0.7829]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:31<02:07,  7.98s/it, train_loss=0.7266, val_loss=0.7829]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:39<02:07,  7.98s/it, train_loss=0.6790, val_loss=0.7452]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:39<01:59,  7.99s/it, train_loss=0.6790, val_loss=0.7452]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:47<01:59,  7.99s/it, train_loss=0.6461, val_loss=0.7059]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:47<01:52,  8.01s/it, train_loss=0.6461, val_loss=0.7059]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:55<01:52,  8.01s/it, train_loss=0.6254, val_loss=0.6787]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [00:55<01:43,  7.99s/it, train_loss=0.6254, val_loss=0.6787]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [01:03<01:43,  7.99s/it, train_loss=0.5864, val_loss=0.7387]\u001b[A\n",
            "  -> Task 1 Training:  40% 8/20 [01:03<01:35,  7.99s/it, train_loss=0.5864, val_loss=0.7387]\u001b[A\n",
            "  -> Task 1 Training:  40% 8/20 [01:11<01:35,  7.99s/it, train_loss=0.5693, val_loss=0.7571]\u001b[A\n",
            "  -> Task 1 Training:  45% 9/20 [01:11<01:27,  7.99s/it, train_loss=0.5693, val_loss=0.7571]\u001b[A\n",
            "  -> Task 1 Training:  45% 9/20 [01:19<01:27,  7.99s/it, train_loss=0.5507, val_loss=0.7475]\u001b[A\n",
            "  -> Task 1 Training:  50% 10/20 [01:19<01:19,  7.98s/it, train_loss=0.5507, val_loss=0.7475]\u001b[A\n",
            "  -> Task 1 Training:  50% 10/20 [01:27<01:19,  7.98s/it, train_loss=0.5284, val_loss=0.7482]\u001b[A\n",
            "  -> Task 1 Training:  55% 11/20 [01:27<01:11,  7.97s/it, train_loss=0.5284, val_loss=0.7482]\u001b[A\n",
            "  -> Task 1 Training:  55% 11/20 [01:35<01:11,  7.97s/it, train_loss=0.5127, val_loss=0.6543]\u001b[A\n",
            "  -> Task 1 Training:  60% 12/20 [01:35<01:03,  8.00s/it, train_loss=0.5127, val_loss=0.6543]\u001b[A\n",
            "  -> Task 1 Training:  60% 12/20 [01:43<01:03,  8.00s/it, train_loss=0.4912, val_loss=0.6612]\u001b[A\n",
            "  -> Task 1 Training:  65% 13/20 [01:43<00:55,  7.97s/it, train_loss=0.4912, val_loss=0.6612]\u001b[A\n",
            "  -> Task 1 Training:  65% 13/20 [01:51<00:55,  7.97s/it, train_loss=0.4786, val_loss=0.8154]\u001b[A\n",
            "  -> Task 1 Training:  70% 14/20 [01:51<00:47,  7.96s/it, train_loss=0.4786, val_loss=0.8154]\u001b[A\n",
            "  -> Task 1 Training:  70% 14/20 [01:59<00:47,  7.96s/it, train_loss=0.4575, val_loss=0.6822]\u001b[A\n",
            "  -> Task 1 Training:  75% 15/20 [01:59<00:39,  7.98s/it, train_loss=0.4575, val_loss=0.6822]\u001b[A\n",
            "  -> Task 1 Training:  75% 15/20 [02:07<00:39,  7.98s/it, train_loss=0.4392, val_loss=0.7182]\u001b[A\n",
            "  -> Task 1 Training:  80% 16/20 [02:07<00:31,  7.98s/it, train_loss=0.4392, val_loss=0.7182]\u001b[A\n",
            "  -> Task 1 Training:  80% 16/20 [02:15<00:31,  7.98s/it, train_loss=0.4348, val_loss=0.6977]\u001b[A\n",
            "                                                                                             \u001b[A    Computing Importance Weights (Ω) for BICL...\n",
            "\n",
            "  -> Task 2 Training:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  -> Task 2 Training:   0% 0/20 [00:11<?, ?it/s, train_loss=1.2284, val_loss=1.7772]\u001b[A\n",
            "  -> Task 2 Training:   5% 1/20 [00:11<03:39, 11.54s/it, train_loss=1.2284, val_loss=1.7772]\u001b[A\n",
            "  -> Task 2 Training:   5% 1/20 [00:23<03:39, 11.54s/it, train_loss=0.6967, val_loss=0.8056]\u001b[A\n",
            "  -> Task 2 Training:  10% 2/20 [00:23<03:29, 11.62s/it, train_loss=0.6967, val_loss=0.8056]\u001b[A\n",
            "  -> Task 2 Training:  10% 2/20 [00:34<03:29, 11.62s/it, train_loss=0.6327, val_loss=0.9936]\u001b[A\n",
            "  -> Task 2 Training:  15% 3/20 [00:34<03:16, 11.57s/it, train_loss=0.6327, val_loss=0.9936]\u001b[A\n",
            "  -> Task 2 Training:  15% 3/20 [00:46<03:16, 11.57s/it, train_loss=0.6159, val_loss=0.7889]\u001b[A\n",
            "  -> Task 2 Training:  20% 4/20 [00:46<03:04, 11.56s/it, train_loss=0.6159, val_loss=0.7889]\u001b[A\n",
            "  -> Task 2 Training:  20% 4/20 [00:57<03:04, 11.56s/it, train_loss=0.6037, val_loss=0.6995]\u001b[A\n",
            "  -> Task 2 Training:  25% 5/20 [00:57<02:53, 11.56s/it, train_loss=0.6037, val_loss=0.6995]\u001b[A\n",
            "  -> Task 2 Training:  25% 5/20 [01:09<02:53, 11.56s/it, train_loss=0.5900, val_loss=0.6716]\u001b[A\n",
            "  -> Task 2 Training:  30% 6/20 [01:09<02:42, 11.58s/it, train_loss=0.5900, val_loss=0.6716]\u001b[A\n",
            "  -> Task 2 Training:  30% 6/20 [01:21<02:42, 11.58s/it, train_loss=0.5850, val_loss=0.6502]\u001b[A\n",
            "  -> Task 2 Training:  35% 7/20 [01:21<02:30, 11.60s/it, train_loss=0.5850, val_loss=0.6502]\u001b[A\n",
            "  -> Task 2 Training:  35% 7/20 [01:32<02:30, 11.60s/it, train_loss=0.5713, val_loss=0.6772]\u001b[A\n",
            "  -> Task 2 Training:  40% 8/20 [01:32<02:18, 11.57s/it, train_loss=0.5713, val_loss=0.6772]\u001b[A\n",
            "  -> Task 2 Training:  40% 8/20 [01:44<02:18, 11.57s/it, train_loss=0.5678, val_loss=0.7478]\u001b[A\n",
            "  -> Task 2 Training:  45% 9/20 [01:44<02:06, 11.53s/it, train_loss=0.5678, val_loss=0.7478]\u001b[A\n",
            "  -> Task 2 Training:  45% 9/20 [01:55<02:06, 11.53s/it, train_loss=0.5591, val_loss=0.6929]\u001b[A\n",
            "  -> Task 2 Training:  50% 10/20 [01:55<01:54, 11.47s/it, train_loss=0.5591, val_loss=0.6929]\u001b[A\n",
            "  -> Task 2 Training:  50% 10/20 [02:06<01:54, 11.47s/it, train_loss=0.5507, val_loss=0.6602]\u001b[A\n",
            "  -> Task 2 Training:  55% 11/20 [02:06<01:42, 11.41s/it, train_loss=0.5507, val_loss=0.6602]\u001b[A\n",
            "  -> Task 2 Training:  55% 11/20 [02:18<01:42, 11.41s/it, train_loss=0.5497, val_loss=0.6156]\u001b[A\n",
            "  -> Task 2 Training:  60% 12/20 [02:18<01:31, 11.47s/it, train_loss=0.5497, val_loss=0.6156]\u001b[A\n",
            "  -> Task 2 Training:  60% 12/20 [02:29<01:31, 11.47s/it, train_loss=0.5445, val_loss=0.7054]\u001b[A\n",
            "  -> Task 2 Training:  65% 13/20 [02:29<01:20, 11.45s/it, train_loss=0.5445, val_loss=0.7054]\u001b[A\n",
            "  -> Task 2 Training:  65% 13/20 [02:41<01:20, 11.45s/it, train_loss=0.5414, val_loss=0.5911]\u001b[A\n",
            "  -> Task 2 Training:  70% 14/20 [02:41<01:08, 11.45s/it, train_loss=0.5414, val_loss=0.5911]\u001b[A\n",
            "  -> Task 2 Training:  70% 14/20 [02:52<01:08, 11.45s/it, train_loss=0.5368, val_loss=0.7378]\u001b[A\n",
            "  -> Task 2 Training:  75% 15/20 [02:52<00:57, 11.42s/it, train_loss=0.5368, val_loss=0.7378]\u001b[A\n",
            "  -> Task 2 Training:  75% 15/20 [03:03<00:57, 11.42s/it, train_loss=0.5319, val_loss=0.6943]\u001b[A\n",
            "  -> Task 2 Training:  80% 16/20 [03:03<00:45, 11.42s/it, train_loss=0.5319, val_loss=0.6943]\u001b[A\n",
            "  -> Task 2 Training:  80% 16/20 [03:15<00:45, 11.42s/it, train_loss=0.5282, val_loss=0.8768]\u001b[A\n",
            "  -> Task 2 Training:  85% 17/20 [03:15<00:34, 11.42s/it, train_loss=0.5282, val_loss=0.8768]\u001b[A\n",
            "  -> Task 2 Training:  85% 17/20 [03:26<00:34, 11.42s/it, train_loss=0.5338, val_loss=0.7424]\u001b[A\n",
            "  -> Task 2 Training:  90% 18/20 [03:26<00:22, 11.40s/it, train_loss=0.5338, val_loss=0.7424]\u001b[A\n",
            "  -> Task 2 Training:  90% 18/20 [03:38<00:22, 11.40s/it, train_loss=0.5286, val_loss=0.7291]\u001b[A\n",
            "                                                                                             \u001b[A    Computing Importance Weights (Ω) for BICL...\n",
            "2025-07-06 02:37:15,806 [INFO    ]     Trial complete: bicl_beta0.005. Avg Acc: 0.385, BWT: -0.758, Time: 353.8s\n",
            "Run 1 | Trial: bicl_beta0.005: 100% 3/3 [14:41<00:00, 293.96s/it]\n",
            "2025-07-06 02:37:15,806 [INFO    ] --- Starting Statistical Run 2/5 ---\n",
            "2025-07-06 02:37:15,806 [INFO    ] Preparing Split CIFAR-10 benchmark...\n",
            "Run 2 | Trial: vanilla:   0% 0/3 [00:00<?, ?it/s]\n",
            "  -> Task 1 Training:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  -> Task 1 Training:   0% 0/20 [00:07<?, ?it/s, train_loss=1.2426, val_loss=1.0531]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:07<02:31,  7.98s/it, train_loss=1.2426, val_loss=1.0531]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:15<02:31,  7.98s/it, train_loss=0.9379, val_loss=0.9695]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:15<02:23,  7.99s/it, train_loss=0.9379, val_loss=0.9695]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:23<02:23,  7.99s/it, train_loss=0.8240, val_loss=0.8280]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:23<02:15,  7.98s/it, train_loss=0.8240, val_loss=0.8280]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:31<02:15,  7.98s/it, train_loss=0.7519, val_loss=0.8047]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:31<02:07,  8.00s/it, train_loss=0.7519, val_loss=0.8047]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:39<02:07,  8.00s/it, train_loss=0.7016, val_loss=0.7678]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:39<02:00,  8.00s/it, train_loss=0.7016, val_loss=0.7678]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:47<02:00,  8.00s/it, train_loss=0.6652, val_loss=0.7616]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:47<01:52,  8.01s/it, train_loss=0.6652, val_loss=0.7616]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:55<01:52,  8.01s/it, train_loss=0.6342, val_loss=0.7674]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [00:55<01:43,  7.95s/it, train_loss=0.6342, val_loss=0.7674]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [01:03<01:43,  7.95s/it, train_loss=0.6083, val_loss=0.7330]\u001b[A\n",
            "  -> Task 1 Training:  40% 8/20 [01:03<01:35,  7.96s/it, train_loss=0.6083, val_loss=0.7330]\u001b[A\n",
            "  -> Task 1 Training:  40% 8/20 [01:11<01:35,  7.96s/it, train_loss=0.5816, val_loss=0.6847]\u001b[A\n",
            "  -> Task 1 Training:  45% 9/20 [01:11<01:27,  7.99s/it, train_loss=0.5816, val_loss=0.6847]\u001b[A\n",
            "  -> Task 1 Training:  45% 9/20 [01:19<01:27,  7.99s/it, train_loss=0.5650, val_loss=0.7447]\u001b[A\n",
            "  -> Task 1 Training:  50% 10/20 [01:19<01:19,  7.96s/it, train_loss=0.5650, val_loss=0.7447]\u001b[A\n",
            "  -> Task 1 Training:  50% 10/20 [01:27<01:19,  7.96s/it, train_loss=0.5468, val_loss=0.6678]\u001b[A\n",
            "  -> Task 1 Training:  55% 11/20 [01:27<01:11,  7.98s/it, train_loss=0.5468, val_loss=0.6678]\u001b[A\n",
            "  -> Task 1 Training:  55% 11/20 [01:35<01:11,  7.98s/it, train_loss=0.5259, val_loss=0.7153]\u001b[A\n",
            "  -> Task 1 Training:  60% 12/20 [01:35<01:03,  7.96s/it, train_loss=0.5259, val_loss=0.7153]\u001b[A\n",
            "  -> Task 1 Training:  60% 12/20 [01:43<01:03,  7.96s/it, train_loss=0.5075, val_loss=0.7056]\u001b[A\n",
            "  -> Task 1 Training:  65% 13/20 [01:43<00:55,  7.98s/it, train_loss=0.5075, val_loss=0.7056]\u001b[A\n",
            "  -> Task 1 Training:  65% 13/20 [01:51<00:55,  7.98s/it, train_loss=0.5054, val_loss=0.6884]\u001b[A\n",
            "  -> Task 1 Training:  70% 14/20 [01:51<00:47,  7.96s/it, train_loss=0.5054, val_loss=0.6884]\u001b[A\n",
            "  -> Task 1 Training:  70% 14/20 [01:59<00:47,  7.96s/it, train_loss=0.4728, val_loss=0.7018]\u001b[A\n",
            "  -> Task 1 Training:  75% 15/20 [01:59<00:39,  7.95s/it, train_loss=0.4728, val_loss=0.7018]\u001b[A\n",
            "  -> Task 1 Training:  75% 15/20 [02:07<00:39,  7.95s/it, train_loss=0.4706, val_loss=0.7070]\u001b[A\n",
            "                                                                                             \u001b[A\n",
            "  -> Task 2 Training:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  -> Task 2 Training:   0% 0/20 [00:07<?, ?it/s, train_loss=1.2442, val_loss=0.7581]\u001b[A\n",
            "  -> Task 2 Training:   5% 1/20 [00:08<02:32,  8.05s/it, train_loss=1.2442, val_loss=0.7581]\u001b[A\n",
            "  -> Task 2 Training:   5% 1/20 [00:15<02:32,  8.05s/it, train_loss=0.5809, val_loss=0.6009]\u001b[A\n",
            "  -> Task 2 Training:  10% 2/20 [00:16<02:24,  8.03s/it, train_loss=0.5809, val_loss=0.6009]\u001b[A\n",
            "  -> Task 2 Training:  10% 2/20 [00:24<02:24,  8.03s/it, train_loss=0.4910, val_loss=0.5060]\u001b[A\n",
            "  -> Task 2 Training:  15% 3/20 [00:24<02:16,  8.06s/it, train_loss=0.4910, val_loss=0.5060]\u001b[A\n",
            "  -> Task 2 Training:  15% 3/20 [00:32<02:16,  8.06s/it, train_loss=0.4467, val_loss=0.4712]\u001b[A\n",
            "  -> Task 2 Training:  20% 4/20 [00:32<02:09,  8.07s/it, train_loss=0.4467, val_loss=0.4712]\u001b[A\n",
            "  -> Task 2 Training:  20% 4/20 [00:40<02:09,  8.07s/it, train_loss=0.4095, val_loss=0.5767]\u001b[A\n",
            "  -> Task 2 Training:  25% 5/20 [00:40<02:00,  8.03s/it, train_loss=0.4095, val_loss=0.5767]\u001b[A\n",
            "  -> Task 2 Training:  25% 5/20 [00:48<02:00,  8.03s/it, train_loss=0.3837, val_loss=0.4858]\u001b[A\n",
            "  -> Task 2 Training:  30% 6/20 [00:48<01:51,  7.98s/it, train_loss=0.3837, val_loss=0.4858]\u001b[A\n",
            "  -> Task 2 Training:  30% 6/20 [00:56<01:51,  7.98s/it, train_loss=0.3558, val_loss=0.4975]\u001b[A\n",
            "  -> Task 2 Training:  35% 7/20 [00:56<01:43,  7.97s/it, train_loss=0.3558, val_loss=0.4975]\u001b[A\n",
            "  -> Task 2 Training:  35% 7/20 [01:03<01:43,  7.97s/it, train_loss=0.3280, val_loss=0.4987]\u001b[A\n",
            "  -> Task 2 Training:  40% 8/20 [01:03<01:35,  7.95s/it, train_loss=0.3280, val_loss=0.4987]\u001b[A\n",
            "  -> Task 2 Training:  40% 8/20 [01:11<01:35,  7.95s/it, train_loss=0.3195, val_loss=0.5966]\u001b[A\n",
            "                                                                                            \u001b[A2025-07-06 02:41:07,624 [INFO    ]     Trial complete: vanilla. Avg Acc: 0.411, BWT: -0.752, Time: 199.4s\n",
            "Run 2 | Trial: bicl_beta0.01:  33% 1/3 [03:24<06:49, 204.82s/it]\n",
            "  -> Task 1 Training:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  -> Task 1 Training:   0% 0/20 [00:07<?, ?it/s, train_loss=1.2600, val_loss=1.0846]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:07<02:31,  7.95s/it, train_loss=1.2600, val_loss=1.0846]\u001b[A\n",
            "  -> Task 1 Training:   5% 1/20 [00:15<02:31,  7.95s/it, train_loss=0.9269, val_loss=0.9831]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:15<02:23,  7.96s/it, train_loss=0.9269, val_loss=0.9831]\u001b[A\n",
            "  -> Task 1 Training:  10% 2/20 [00:23<02:23,  7.96s/it, train_loss=0.8073, val_loss=0.8644]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:23<02:15,  8.00s/it, train_loss=0.8073, val_loss=0.8644]\u001b[A\n",
            "  -> Task 1 Training:  15% 3/20 [00:31<02:15,  8.00s/it, train_loss=0.7261, val_loss=0.8161]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:31<02:08,  8.01s/it, train_loss=0.7261, val_loss=0.8161]\u001b[A\n",
            "  -> Task 1 Training:  20% 4/20 [00:39<02:08,  8.01s/it, train_loss=0.6849, val_loss=0.7631]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:40<02:00,  8.02s/it, train_loss=0.6849, val_loss=0.7631]\u001b[A\n",
            "  -> Task 1 Training:  25% 5/20 [00:47<02:00,  8.02s/it, train_loss=0.6430, val_loss=0.9694]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:47<01:51,  7.98s/it, train_loss=0.6430, val_loss=0.9694]\u001b[A\n",
            "  -> Task 1 Training:  30% 6/20 [00:55<01:51,  7.98s/it, train_loss=0.6151, val_loss=0.7170]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [00:55<01:43,  8.00s/it, train_loss=0.6151, val_loss=0.7170]\u001b[A\n",
            "  -> Task 1 Training:  35% 7/20 [01:03<01:43,  8.00s/it, train_loss=0.5822, val_loss=0.8136]\u001b[A\n",
            "  -> Task 1 Training:  40% 8/20 [01:03<01:35,  7.98s/it, train_loss=0.5822, val_loss=0.8136]\u001b[A\n",
            "Run 2 | Trial: bicl_beta0.01:  33% 1/3 [04:29<08:59, 269.89s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/bicl-framework/scripts/run_experiment.py\", line 144, in <module>\n",
            "    main(args)\n",
            "  File \"/content/bicl-framework/scripts/run_experiment.py\", line 98, in main\n",
            "    accuracies, bwt_results, complexity = runner.run_comprehensive_experiment()\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/bicl-framework/src/experiment.py\", line 256, in run_comprehensive_experiment\n",
            "    self._train_one_task(model, train_ds, method, cl_framework, task_idx, checkpoint_path)\n",
            "  File \"/content/bicl-framework/src/experiment.py\", line 103, in _train_one_task\n",
            "    total_loss.backward()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 648, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 353, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}